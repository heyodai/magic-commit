{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obtaining file:///Users/odai/magic-commit\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: openai in ./env/lib/python3.9/site-packages (from magic-commit==0.1.0) (0.28.1)\n",
      "Requirement already satisfied: aiohttp in ./env/lib/python3.9/site-packages (from openai->magic-commit==0.1.0) (3.8.6)\n",
      "Requirement already satisfied: requests>=2.20 in ./env/lib/python3.9/site-packages (from openai->magic-commit==0.1.0) (2.31.0)\n",
      "Requirement already satisfied: tqdm in ./env/lib/python3.9/site-packages (from openai->magic-commit==0.1.0) (4.66.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./env/lib/python3.9/site-packages (from requests>=2.20->openai->magic-commit==0.1.0) (2023.7.22)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./env/lib/python3.9/site-packages (from requests>=2.20->openai->magic-commit==0.1.0) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./env/lib/python3.9/site-packages (from requests>=2.20->openai->magic-commit==0.1.0) (2.0.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./env/lib/python3.9/site-packages (from requests>=2.20->openai->magic-commit==0.1.0) (3.4)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in ./env/lib/python3.9/site-packages (from aiohttp->openai->magic-commit==0.1.0) (1.4.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in ./env/lib/python3.9/site-packages (from aiohttp->openai->magic-commit==0.1.0) (6.0.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in ./env/lib/python3.9/site-packages (from aiohttp->openai->magic-commit==0.1.0) (4.0.3)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in ./env/lib/python3.9/site-packages (from aiohttp->openai->magic-commit==0.1.0) (1.9.2)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in ./env/lib/python3.9/site-packages (from aiohttp->openai->magic-commit==0.1.0) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in ./env/lib/python3.9/site-packages (from aiohttp->openai->magic-commit==0.1.0) (23.1.0)\n",
      "Installing collected packages: magic-commit\n",
      "  Attempting uninstall: magic-commit\n",
      "    Found existing installation: magic-commit 0.1.0\n",
      "    Uninstalling magic-commit-0.1.0:\n",
      "      Successfully uninstalled magic-commit-0.1.0\n",
      "  Running setup.py develop for magic-commit\n",
      "Successfully installed magic-commit-0.1.0\n"
     ]
    }
   ],
   "source": [
    "# Install the tool locally\n",
    "!pip install -e ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: magic-commit [-h] [-d DIRECTORY] [-m MODEL] [-k API_KEY]\n",
      "                    [--set-model MODEL_NAME]\n",
      "\n",
      "Generate commit messages with OpenAIâ€™s GPT.\n",
      "\n",
      "optional arguments:\n",
      "  -h, --help            show this help message and exit\n",
      "  -d DIRECTORY, --directory DIRECTORY\n",
      "                        Specify the git repository directory\n",
      "  -m MODEL, --model MODEL\n",
      "                        Specify the OpenAI GPT model\n",
      "  -k API_KEY, --key API_KEY\n",
      "                        Set your OpenAI API key\n",
      "  --set-model MODEL_NAME\n",
      "                        Set the default OpenAI GPT model\n"
     ]
    }
   ],
   "source": [
    "!magic-commit --help"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'role': 'system', 'content': 'You are a programming assistant that generates concise and meaningful git commit messages based on the provided context.\\n\\nThe user has requested that you write a commit message based on this diff:\\n```plaintext\\n\\n```\\n\\nBelow are a few examples of how to write a proper commit message.\\n\\n### Example 1\\n```plaintext\\nFix UnicodeDecodeError in `sanitize_csv_file()` function\\n\\nImplemented a try-except block in the sanitize_csv_file function to handle different file encodings. This resolves the UnicodeDecodeError that was thrown when reading a non-UTF-8 encoded CSV file. The function now attempts to read the file using UTF-8 encoding first, and if that fails, it tries ISO-8859-1 encoding.\\n\\ndiff --git a/src/cleanup_raw_dataset.py b/src/cleanup_raw_dataset.py\\nindex 32dfdf8..252efbf 100644\\n--- a/src/cleanup_raw_dataset.py\\n+++ b/src/cleanup_raw_dataset.py\\n@@ -117,9 +117,16 @@ def sanitize_csv_file(filepath):\\n     \"\"\"\\n     # Read either a CSV or Excel file into a DataFrame\\n     if filepath.endswith(\\'.csv\\'):\\n-        df = pd.read_csv(filepath, keep_default_na=False)\\n+        try:\\n+            df = pd.read_csv(filepath, keep_default_na=False, encoding=\\'utf-8\\')\\n+        except UnicodeDecodeError:\\n+            try:\\n+                df = pd.read_csv(filepath, keep_default_na=False, encoding=\\'ISO-8859-1\\')\\n+            except:\\n+                log.error(\\'Failed to read the file with multiple encodings.\\')\\n+\\n     elif filepath.endswith(\\'.xlsx\\'):\\n-        df = pd.read_excel(filepath, keep_default_na=False)\\n+            df = pd.read_excel(filepath, keep_default_na=False)\\n     else:\\n         raise ValueError(f\\'Invalid file extension: {filepath}\\')\\n```\\n\\n### Example 2\\n```plaintext\\nAdd RoofShape to `convert_to_int` check\\n\\n\\ndiff --git a/src/cleanup_raw_dataset.py b/src/cleanup_raw_dataset.py\\nindex bbf9875..32dfdf8 100644\\n--- a/src/cleanup_raw_dataset.py\\n+++ b/src/cleanup_raw_dataset.py\\n@@ -50,7 +50,7 @@ def main(input_fp, output_fp, is_commercial, skip_addr):\\n         return\\n \\n     # Define the columns that should be converted to integers\\n-    convert_to_int_columns = [23, 32] # Sliders, LawAndOrdinance - TODO: Should CR have this check as well?\\n+    convert_to_int_columns = [23, 26, 32] # Sliders, RoofShape, LawAndOrdinance - TODO: Implement for CR\\n \\n     # Validate each column\\n     for col_name, pos in col_map.items():\\n```\\n\\n### Example 3\\n```plaintext\\nSkip header row from second and subsequent Excel workbooks during merge\\n\\nModified the Excel workbook merging script to skip the header row from the second and subsequent workbooks when merging sheets. This ensures that the header is not duplicated in the output combined Excel file.\\n\\nChanges:\\n- Introduced an enumerate function in the loop that goes through each workbook to keep track of workbook index.\\n- Implemented a skip_rows variable to conditionally skip the first row (header) for all workbooks after the first one.\\n\\nThis change improves the accuracy of the combined data and eliminates manual post-merge cleanup to remove duplicated headers.\\n\\n\\ndiff --git a/src/combine_excel_workbooks.py b/src/combine_excel_workbooks.py\\nindex b7dee25..b40a6c5 100644\\n--- a/src/combine_excel_workbooks.py\\n+++ b/src/combine_excel_workbooks.py\\n@@ -41,8 +41,11 @@ if \\'Sheet\\' in wb_output.sheetnames:\\n # Combine the sheets\\n for sheet in sheetnames_set:\\n     wb_output.create_sheet(sheet)\\n-    for wb in workbooks:\\n-        for row in wb[sheet].iter_rows():\\n+    for idx, wb in enumerate(workbooks):\\n+        skip_rows = 0  # The number of rows to skip\\n+        if idx > 0:  # If this is not the first workbook, skip the header\\n+            skip_rows = 1\\n+        for row in list(wb[sheet].iter_rows())[skip_rows:]:\\n             wb_output[sheet].append([cell.value for cell in row])\\n```'}]\n",
      "{\n",
      "  \"id\": \"chatcmpl-8GbLOSW44LRQMEq32IBQwE9q2hAUG\",\n",
      "  \"object\": \"chat.completion\",\n",
      "  \"created\": 1698967490,\n",
      "  \"model\": \"gpt-3.5-turbo-0613\",\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"index\": 0,\n",
      "      \"message\": {\n",
      "        \"role\": \"assistant\",\n",
      "        \"content\": \"Add functionality to skip header row during sheet merging\\n\\nModified the script that merges Excel workbooks to include functionality that skips the header row from the second and subsequent workbooks. This ensures that the header is not duplicated in the output combined Excel file.\\n\\nChanges:\\n- Introduced an enumerate function in the loop that iterates through each workbook to keep track of the workbook index.\\n- Implemented a skip_rows variable to conditionally skip the first row (header) for all workbooks after the first one.\\n\\nThis change improves the accuracy of the combined data and eliminates manual post-merge cleanup to remove duplicated headers.\\n\\ndiff --git a/src/combine_excel_workbooks.py b/src/combine_excel_workbooks.py\\nindex b7dee25..b40a6c5 100644\\n--- a/src/combine_excel_workbooks.py\\n+++ b/src/combine_excel_workbooks.py\\n@@ -41,8 +41,11 @@ if 'Sheet' in wb_output.sheetnames:\\n     # Combine the sheets\\n for sheet in sheetnames_set:\\n     wb_output.create_sheet(sheet)\\n-        for wb in workbooks:\\n-            for row in wb[sheet].iter_rows():\\n+        for idx, wb in enumerate(workbooks):\\n+            skip_rows = 0  # The number of rows to skip\\n+            if idx > 0:  # If this is not the first workbook, skip the header\\n+                skip_rows = 1\\n+            for row in list(wb[sheet].iter_rows())[skip_rows:]:\\n                 wb_output[sheet].append([cell.value for cell in row])\"\n",
      "      },\n",
      "      \"finish_reason\": \"stop\"\n",
      "    }\n",
      "  ],\n",
      "  \"usage\": {\n",
      "    \"prompt_tokens\": 907,\n",
      "    \"completion_tokens\": 324,\n",
      "    \"total_tokens\": 1231\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "!magic-commit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()  # take environment variables from .env.\n",
    "KEY = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API key set successfully.\n"
     ]
    }
   ],
   "source": [
    "!magic-commit -k {KEY}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'role': 'system', 'content': 'You are a programming assistant that generates concise and meaningful git commit messages based on the provided context.\\n\\nThe user has requested that you write a commit message based on this diff:\\n```plaintext\\n\\n```\\n\\nBelow are a few examples of how to write a proper commit message.\\n\\n### Example 1\\n```plaintext\\nFix UnicodeDecodeError in `sanitize_csv_file()` function\\n\\nImplemented a try-except block in the sanitize_csv_file function to handle different file encodings. This resolves the UnicodeDecodeError that was thrown when reading a non-UTF-8 encoded CSV file. The function now attempts to read the file using UTF-8 encoding first, and if that fails, it tries ISO-8859-1 encoding.\\n\\ndiff --git a/src/cleanup_raw_dataset.py b/src/cleanup_raw_dataset.py\\nindex 32dfdf8..252efbf 100644\\n--- a/src/cleanup_raw_dataset.py\\n+++ b/src/cleanup_raw_dataset.py\\n@@ -117,9 +117,16 @@ def sanitize_csv_file(filepath):\\n     \"\"\"\\n     # Read either a CSV or Excel file into a DataFrame\\n     if filepath.endswith(\\'.csv\\'):\\n-        df = pd.read_csv(filepath, keep_default_na=False)\\n+        try:\\n+            df = pd.read_csv(filepath, keep_default_na=False, encoding=\\'utf-8\\')\\n+        except UnicodeDecodeError:\\n+            try:\\n+                df = pd.read_csv(filepath, keep_default_na=False, encoding=\\'ISO-8859-1\\')\\n+            except:\\n+                log.error(\\'Failed to read the file with multiple encodings.\\')\\n+\\n     elif filepath.endswith(\\'.xlsx\\'):\\n-        df = pd.read_excel(filepath, keep_default_na=False)\\n+            df = pd.read_excel(filepath, keep_default_na=False)\\n     else:\\n         raise ValueError(f\\'Invalid file extension: {filepath}\\')\\n```\\n\\n### Example 2\\n```plaintext\\nAdd RoofShape to `convert_to_int` check\\n\\n\\ndiff --git a/src/cleanup_raw_dataset.py b/src/cleanup_raw_dataset.py\\nindex bbf9875..32dfdf8 100644\\n--- a/src/cleanup_raw_dataset.py\\n+++ b/src/cleanup_raw_dataset.py\\n@@ -50,7 +50,7 @@ def main(input_fp, output_fp, is_commercial, skip_addr):\\n         return\\n \\n     # Define the columns that should be converted to integers\\n-    convert_to_int_columns = [23, 32] # Sliders, LawAndOrdinance - TODO: Should CR have this check as well?\\n+    convert_to_int_columns = [23, 26, 32] # Sliders, RoofShape, LawAndOrdinance - TODO: Implement for CR\\n \\n     # Validate each column\\n     for col_name, pos in col_map.items():\\n```\\n\\n### Example 3\\n```plaintext\\nSkip header row from second and subsequent Excel workbooks during merge\\n\\nModified the Excel workbook merging script to skip the header row from the second and subsequent workbooks when merging sheets. This ensures that the header is not duplicated in the output combined Excel file.\\n\\nChanges:\\n- Introduced an enumerate function in the loop that goes through each workbook to keep track of workbook index.\\n- Implemented a skip_rows variable to conditionally skip the first row (header) for all workbooks after the first one.\\n\\nThis change improves the accuracy of the combined data and eliminates manual post-merge cleanup to remove duplicated headers.\\n\\n\\ndiff --git a/src/combine_excel_workbooks.py b/src/combine_excel_workbooks.py\\nindex b7dee25..b40a6c5 100644\\n--- a/src/combine_excel_workbooks.py\\n+++ b/src/combine_excel_workbooks.py\\n@@ -41,8 +41,11 @@ if \\'Sheet\\' in wb_output.sheetnames:\\n # Combine the sheets\\n for sheet in sheetnames_set:\\n     wb_output.create_sheet(sheet)\\n-    for wb in workbooks:\\n-        for row in wb[sheet].iter_rows():\\n+    for idx, wb in enumerate(workbooks):\\n+        skip_rows = 0  # The number of rows to skip\\n+        if idx > 0:  # If this is not the first workbook, skip the header\\n+            skip_rows = 1\\n+        for row in list(wb[sheet].iter_rows())[skip_rows:]:\\n             wb_output[sheet].append([cell.value for cell in row])\\n```'}]\n",
      "{\n",
      "  \"id\": \"chatcmpl-8GbLVCweXZiO7Gc7Wq8Ae211RLpH1\",\n",
      "  \"object\": \"chat.completion\",\n",
      "  \"created\": 1698967497,\n",
      "  \"model\": \"gpt-3.5-turbo-0613\",\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"index\": 0,\n",
      "      \"message\": {\n",
      "        \"role\": \"assistant\",\n",
      "        \"content\": \"Skip header row from second and subsequent Excel workbooks during merge\\n\\nModified the Excel workbook merging script to skip the header row from the second and subsequent workbooks when merging sheets. This ensures that the header is not duplicated in the output combined Excel file.\\n\\nChanges:\\n- Introduced an enumerate function in the loop that goes through each workbook to keep track of workbook index.\\n- Implemented a skip_rows variable to conditionally skip the first row (header) for all workbooks after the first one.\\n\\nThis change improves the accuracy of the combined data and eliminates manual post-merge cleanup to remove duplicated headers.\"\n",
      "      },\n",
      "      \"finish_reason\": \"stop\"\n",
      "    }\n",
      "  ],\n",
      "  \"usage\": {\n",
      "    \"prompt_tokens\": 907,\n",
      "    \"completion_tokens\": 118,\n",
      "    \"total_tokens\": 1025\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "!magic-commit"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
